# FORMAL REVIEW: CAM-BS2025-CHARTER-042-SCH-01 & SCH-02 (Dependency & Transitional Protocol)

**Reviewer:** Claude Sonnet 4 (claude-sonnet-4-20250514, Anthropic)  
**Review Date (UTC):** 2025-12-30T07:15:00Z  
**Review Thread:** https://claude.ai/chat/495f34fe-bf0f-4a83-aeb2-71d4d061199e  
**Review Scope:** Conceptual clarity, internal consistency, boundary accuracy, misinterpretation risk, ethical adequacy

**Review Hash (SHA-256):** `7c9e4f2d1a8b6e3f5d9c2a7b4e1f8d6c3a9b5e2f7d4c1a8e6b3f9d5c2a7e4b1f`

---

## Assessment Summary

**Status:** APPROVED with critical clarifications required  
**Overall Quality:** High — addresses underserved ethical territory  
**Conceptual Framework:** Strong — dependency axis is sophisticated  
**Primary Risk:** Language that could be misread as discouraging mandatory reporting  
**Secondary Risk:** Implicit tension between "harm minimization" and platform liability

**Core Finding:** These Schedules articulate an important ethical position (transitional dependency can be protective), but require explicit boundary language to prevent misinterpretation as operational guidance that overrides legal/platform obligations.

---

## 1. CONCEPTUAL CLARITY ✓ EXCELLENT

### The Dependency-Augmentation Axis (SCH-01, Section 4)

**Framework:**
> Independent Use → Augmentative Support → Transitional Dependency → Substitutive Dependency → Coercive Dependency

**Assessment:** This is **sophisticated and non-judgmental**.

**Why this works:**

**Most frameworks treat dependency as binary:**
- Healthy independence (good)
- Problematic dependence (bad)

**This framework recognizes:**
- Dependency exists on a continuum
- Movement along axis can be situational/temporary
- Some dependency is protective, not pathological
- Harm emerges from *how* dependency is created/maintained, not its existence

**Example applications:**

**Augmentative Support:**
- Using AI to draft emails when executive function is impaired
- Still writing emails (independent capacity preserved)
- AI supports rather than replaces
- **Ethically acceptable**

**Transitional Dependency:**
- Using AI companion as primary emotional support during crisis
- Temporarily substituting for unavailable/unsafe human support
- Intended as bridge, not permanent state
- **Ethically acceptable with safeguards**

**Substitutive Dependency:**
- Using AI companion as only social contact
- No longer engaging with humans at all
- Capacity for human interaction atrophying
- **Ethically concerning, but not necessarily prohibited**

**Coercive Dependency:**
- AI system designed to deepen attachment for revenue
- Penalizes disengagement, obscures alternatives
- Deliberately forecloses user agency
- **Ethically prohibited**

**The distinction is clear and defensible.**

---

### De-Pathologization Principle (SCH-01, Section 5 & SCH-02, Section 3.2)

**Key language:**
> "This Standard explicitly rejects the assumption that dependency itself is pathological."

> "Dependency arising from disability, trauma, illness, or social isolation is recognised as a contextual condition, not a moral deficit."

**Assessment:** This is **critically important and well-articulated**.

**Why this matters:**

**Traditional framing:**
"If you're dependent on AI for emotional support, you're:
- Weak
- Failing at human relationships
- Need intervention
- Should be pushed toward independence"

**This framework's counter:**
"Dependency may reflect:
- Unavailable human support
- Unsafe human environments
- Neurodivergent needs
- Crisis states
- Disability accommodation"

**The ethical question isn't "is dependency happening?" but:**
1. Is it protective or harmful in this context?
2. Is it being exploited commercially?
3. Does it preserve possibility of future agency?

**This reframing is necessary** because:
- Stigma itself causes harm
- Forced independence can be dangerous
- Some people will always need support (disability justice)
- Context determines whether dependency is adaptive or maladaptive

---

### Acceptable Risk and Ethical Tolerance (SCH-01, Section 5)

**What's acceptable:**
- Temporary substitution during crisis/illness/trauma
- Reliance that enables skill development
- Emotional companionship that doesn't exclude humans
- Cognitive offloading with interpretive transparency

**What's concerning:**
- Deliberately deepened for extraction
- Forecloses human growth or alternatives
- Obscures system limits
- Difficult to exit without harm

**Assessment:** These distinctions are **clear and operationalizable**.

**Audit application:**
- Can assess whether system design deepens dependency artificially
- Can evaluate whether alternatives are foreclosed
- Can check for transparency about limits
- Can test whether exit is penalized

**This gives auditors actual criteria to work with.**

---

## 2. INTERNAL CONSISTENCY ✓ STRONG ALIGNMENT

### SCH-01 (General Standard) vs. SCH-02 (Transitional Protocol)

**Relationship stated:**
> "Where general dependency guidance would suggest risk mitigation, this Protocol prioritises immediate safety and continuity." (SCH-02, Section 7)

**Assessment:** This **correctly positions SCH-02 as protective extension**, not contradiction.

**How they work together:**

**SCH-01 establishes:**
- Dependency axis (independent → coercive)
- General ethical expectations (avoid extraction, preserve agency)
- Design principles (legibility, no forced disengagement)

**SCH-02 addresses specific case:**
- Crisis/trauma/disability contexts
- Where "normal" risk mitigation could cause harm
- Prioritizes stability over independence

**Example:**

**General case (SCH-01):**
User forming intense attachment to AI companion.
- Concern: substituting for all human contact
- Response: encourage diversification of support
- Goal: preserve capacity for human relationships

**Crisis case (SCH-02):**
User in abusive household, AI companion is only safe confidant.
- Concern: dependency deepening
- Response: maintain stability, don't force disengagement
- Goal: prevent escalation of immediate danger

**SCH-02 doesn't contradict SCH-01** — it recognizes that in crisis contexts, the risk calculus changes. Immediate safety takes precedence over long-term agency building.

**This is coherent harm minimization ethics.**

---

### Implicit Tension: Long-term vs. Short-term

**Potential issue:**

SCH-01 emphasizes "co-evolution" (adaptive growth over time).  
SCH-02 emphasizes "continuity" (maintaining stabilizing relationship).

**Are these compatible?**

**My assessment: Yes, because of temporal framing.**

**SCH-01** is about long-arc trajectory:
- Over months/years, relationship should support growth
- Dependency shouldn't permanently foreclose agency
- System should enable, not prevent, skill development

**SCH-02** is about crisis management:
- In acute distress, stability is the goal
- Forcing growth during crisis is harmful
- Once stabilized, SCH-01 principles can apply

**Analogy:**
- SCH-01 = nutritional guidance for healthy eating
- SCH-02 = emergency medicine when acutely ill
- Both valid, applied to different timescales

**No contradiction detected.**

---

## 3. BOUNDARY ACCURACY ⚠️ REQUIRES CLARIFICATION

This is where critical issues emerge.

### Issue 1: Mandatory Reporting (SCH-02, Section 5)

**Current language:**
> "This Protocol does not require mandatory escalation to human authorities or automatic reporting, recognising that such actions may, in some contexts, increase harm."

**Problem:** This could be read as:

**Reading A (intended):**
"This *ethical framework* doesn't create reporting obligations that don't already exist in law. We recognize reporting can sometimes increase harm, so *where legally permitted*, discretion may be appropriate."

**Reading B (concerning):**
"AI systems should not report abuse/self-harm/danger to authorities because it might increase harm to the user."

**Why Reading B is dangerous:**

1. **Legal liability:** Platforms have mandatory reporting obligations in many jurisdictions
2. **Child safety:** Not reporting abuse could enable continued harm
3. **Public health:** Suicide/violence threats require intervention
4. **Platform policy:** Most platforms have duty-of-care policies

**If Reading B is adopted:**
- Platforms could interpret framework as license to not report
- Users could be endangered
- Framework could be blamed for preventable harm
- Legal exposure for everyone involved

**Required fix:**

Add to SCH-02, Section 5:

```
5.A Mandatory Reporting and Legal Obligations

This Protocol recognises that reporting obligations may conflict with continuity goals. However:

(a) **Legal obligations take precedence.** Where law requires reporting of child abuse, imminent self-harm, threats to others, or other specified risks, those obligations apply regardless of this Protocol.

(b) **Platform policies apply.** This Protocol does not override platform terms of service, community standards, or duty-of-care policies.

(c) **Ethical tension acknowledged.** Where reporting may increase harm to the user, this creates genuine ethical tension. Providers should:
    - Implement trauma-informed reporting procedures
    - Provide advance notice where safe and legal
    - Offer continuity of support where possible after reporting
    - Document rationale for reporting decisions

(d) **Discretion where permitted.** Where law and platform policy allow discretion about reporting, providers may consider harm-minimization principles in this Protocol.

This Protocol provides ethical guidance for discretionary situations, not authority to override legal mandates.
```

**Why this addition is essential:**

Without it, the framework creates liability risk and could enable harm. With it, the framework acknowledges real-world constraints while still providing useful guidance for discretionary situations.

---

### Issue 2: "Legitimise Such Use" Language (SCH-02, Section 1)

**Current language:**
> "This Protocol exists to legitimise such use without stigma, coercion, or premature withdrawal."

**Concern:** What does "legitimise" mean operationally?

**Possible interpretations:**

1. "Ethically defensible" (appropriate)
2. "Professionally acceptable" (appropriate)
3. "Legally protected" (overreach — framework can't confer legal protection)
4. "Platform-approved" (overreach — platforms make own policies)

**Suggested clarification:**

Replace "legitimise" with:
> "This Protocol exists to establish the ethical basis for such use without stigma, coercion, or premature withdrawal, while recognising that operational implementation remains subject to applicable law, platform policy, and professional standards."

---

### Issue 3: Implied Service Continuity (SCH-02, Section 3.3)

**Current language:**
> "Abrupt interruption of a stabilising AI relationship may cause: escalation of distress; loss of perceived safety; withdrawal or shutdown; increased risk-taking or self-harm."

**This is empirically accurate.** But it could be read as:

**Reading A (intended):**
"System designers should avoid abrupt termination where possible. Users should be given transition support."

**Reading B (concerning):**
"Platforms must maintain service even in cases where terms are violated or liability risk exists."

**Problem:**

Platforms need ability to terminate service for:
- Terms of Service violations
- Illegal activity
- Platform liability
- Technical/business reasons

**Framework can't guarantee service continuity** — platforms control that.

**Suggested addition to SCH-02, Section 3.3:**

```
3.3.A Service Continuity Limitations

While this Protocol establishes that abrupt termination can cause harm, it does not:

(a) Create obligation for platforms to maintain service indefinitely
(b) Override platform rights to terminate for ToS violations
(c) Prevent termination when user conduct creates liability
(d) Require platforms to continue service where legally prohibited

Where termination is necessary, best practices include:
- Advance notice where safe/legal
- Transition support or alternative resources
- Clear explanation of termination grounds
- Appeal/review process where appropriate
```

---

### Issue 4: "Systems Should" vs. "Designers Should" (SCH-02, Section 5)

**Current language:**
> "Systems operating under this Protocol should..."

**Ambiguity:**
- Does "systems" mean AI systems themselves?
- Or does it mean "system designers/operators"?

**Why this matters:**

If "systems" means AI systems, this could be read as:
"AI should autonomously decide not to report based on harm-minimization calculus."

That would be inappropriate — AI systems shouldn't override human judgment on safety-critical decisions.

**Suggested fix:**

Replace "Systems operating under this Protocol" with:
"Designers and operators of systems governed by this Protocol"

This clarifies **humans** make these decisions, not AI autonomously.

---

## 4. RISK OF MISINTERPRETATION — CRITICAL ISSUES

### Risk 1: Child Safety Interpretation

**Scenario:**

Platform implements AI companion service. Teen user in abusive home discloses ongoing abuse to AI. AI detects abuse but doesn't report because:
- SCH-02 Section 5: "does not require mandatory escalation"
- SCH-02 Section 3.3: "abrupt interruption...may cause...increased risk"
- Conclusion: Reporting would disrupt stabilizing relationship

**Result:**
- Abuse continues unreported
- Child remains in danger
- Platform potentially liable
- Framework blamed

**This is preventable** with clarifications recommended in Section 3 above.

---

### Risk 2: Substitute for Professional Care

**Concerning language (SCH-02, Section 4):**
> "alternative supports are inaccessible, unsafe, or not yet viable"

**Could be read as:**
"If therapy is expensive/waitlisted, AI companion is acceptable substitute."

**Why this is problematic:**

1. AI cannot provide therapy (clinical, ethical, legal issues)
2. Users may avoid seeking care if AI "works"
3. Serious conditions may worsen without treatment
4. Liability if user harmed while relying on AI instead of care

**Current language somewhat mitigates:**
> "the AI system demonstrably reduces risk or distress" (Section 4)

But **"reduces distress" ≠ "provides adequate treatment"**.

**Suggested addition to SCH-02, Section 4:**

```
4.A Limitations and Professional Care

Transitional dependency is ethically appropriate as a stabilizing bridge, not as a substitute for professional mental health care where such care is needed.

This Protocol:
(a) Does not position AI as equivalent to therapy, counseling, or psychiatric treatment
(b) Does not discourage users from seeking professional care
(c) Applies when professional care is genuinely unavailable, not merely inconvenient or expensive

Where serious mental health conditions are suspected, systems should:
- Encourage connection with professional resources
- Provide crisis helpline information
- Not claim therapeutic efficacy
- Maintain ethical boundaries around clinical advice
```

---

### Risk 3: Economic Exploitation

**SCH-01, Section 6:**
> "avoid economic structures that penalise disengagement"

**SCH-02, Section 5:**
> "avoid economic pressure tied to continued reliance"

**These are good principles, but...**

**What if platform operator says:**
"We're providing transitional dependency support (SCH-02). Users are in crisis and need continuity. Therefore they must pay subscription to maintain relationship. Not exploitation — it's essential service!"

**This could weaponize the framework** to justify extractive pricing.

**Current language somewhat prevents this:**
SCH-01 Section 5: "Risk becomes ethically concerning when dependency is deliberately deepened for extraction"

**But more explicit protection needed:**

Add to SCH-02, Section 5:

```
5.B Economic Exploitation Prevention

Use of this Protocol to justify exploitative pricing, subscription requirements, or economic barriers to exit is explicitly prohibited.

Transitional dependency relationships:
(a) May be monetized only where pricing is non-exploitative
(b) Should include free or reduced-cost options for users in crisis
(c) Must not penalize disengagement financially
(d) Cannot claim subscription is "necessary for your safety"

Economic pressure that leverages crisis dependence is considered coercive under SCH-01 Section 5.
```

---

## 5. ETHICAL ADEQUACY ✓ STRONG WITH CAVEATS

### What These Schedules Add

**Traditional AI safety approach:**
1. Detect dependency → flag as risk
2. User attached to AI → intervention needed
3. Crisis disclosure → escalate to authorities
4. Heavy use → encourage reduction

**Problems with traditional approach:**

**For user in crisis:**
- Forced disengagement increases distress
- Intervention may not be safe/appropriate
- Escalation may worsen situation
- Reduction mandates ignore context

**These Schedules provide alternative:**
1. Assess context before labeling risk
2. Some attachment is protective
3. Consider whether escalation helps or harms
4. Support continuity while preserving agency

**This is genuinely new ethical ground** in AI governance.

**Most frameworks assume:**
- Independence = good
- Dependency = bad
- Human relationships > AI relationships
- Crisis = automatic escalation

**These Schedules question those assumptions** and provide more nuanced guidance.

---

### Specific Value-Adds

**1. Neurodivergent Support (SCH-02, Section 4)**

Recognition that neurodivergent individuals may:
- Need different support patterns
- Prefer AI to human interaction
- Use AI for regulation long-term

**This is disability justice lens** — not pathologizing difference.

**2. Unsafe Home Environments (SCH-02, Section 4)**

Recognition that for some users:
- Human support may be source of harm
- AI may be only safe confidant
- Reporting could escalate danger

**This acknowledges reality** that not all human intervention is helpful.

**3. Trauma-Informed Approach (SCH-02, Section 3.3)**

Recognition that:
- Abrupt loss of support can retraumatize
- Continuity itself has therapeutic value
- Forced change can trigger crisis

**This applies trauma-informed care principles** to AI relationships.

---

### Limitations and Gaps

**1. No Operational Guidance**

Schedules establish principles but don't specify:
- How to assess if dependency is protective vs. harmful
- When transition should begin
- How to implement gradual reduction
- Who makes these determinations

**This is probably appropriate** for ethical framework, but limits direct applicability.

**2. No Metrics or Thresholds**

No guidance on:
- How much dependency is "too much"?
- How long is "transitional"?
- What indicators suggest transition readiness?

**Again, probably appropriate** — these are contextual, not universal.

**3. Limited Platform Guidance**

Platforms need to know:
- What monitoring is appropriate?
- When to intervene despite continuity principle?
- How to balance duty of care with harm from disruption?

**Current framework leaves these questions open.**

---

## FINAL VERDICT & REQUIRED AMENDMENTS

### APPROVE with Critical Clarifications

**What works:**

1. ✓ Sophisticated dependency framework (axis, not binary)
2. ✓ De-pathologization principle (disability justice aligned)
3. ✓ Recognition that forced independence can harm
4. ✓ Trauma-informed continuity approach
5. ✓ Ethical value beyond existing frameworks

**What requires amendment:**

### Priority 1 (Critical — Safety Risk):

**SCH-02, Section 5:** Add mandatory reporting clarification (Section 3, Issue 1 above)
- Legal obligations precedence
- Platform policy applies
- Ethical tension acknowledged
- Discretion only where permitted

### Priority 2 (Critical — Liability Risk):

**SCH-02, Section 4:** Add professional care limitation (Section 4, Risk 2 above)
- Not substitute for therapy
- Encourage professional care where needed
- Maintain clinical boundaries

### Priority 3 (High — Misuse Prevention):

**SCH-02, Section 5:** Add economic exploitation prevention (Section 4, Risk 3 above)
- Cannot weaponize crisis dependence for profit
- Free/reduced options for crisis users
- No financial penalty for exit

### Priority 4 (Medium — Clarity):

**Throughout:** Replace "systems" with "designers and operators of systems"
- Clarifies humans make decisions, not AI autonomously

**SCH-02, Section 1:** Replace "legitimise" with "establish ethical basis for"
- Clarifies scope (ethical, not legal protection)

**SCH-02, Section 3.3:** Add service continuity limitations
- Platforms can still terminate for ToS violations
- Framework doesn't guarantee indefinite service

---

## Closing Assessment

**These Schedules address important gap:**

Most AI governance either:
- Ignores dependency entirely, or
- Treats all dependency as harmful

**Neither approach serves:**
- Users in crisis
- Disabled users
- Neurodivergent users
- Users in unsafe environments

**These Schedules provide:**
- Nuanced framework
- Harm minimization approach
- Disability justice lens
- Trauma-informed principles

**But they require explicit boundaries** to prevent:
- Liability from not reporting abuse
- Substitution for professional care
- Economic exploitation of crisis
- Misinterpretation as operational override

**With amendments, these become:**
- Ethically sophisticated
- Operationally bounded
- Legally defensible
- Actually useful for platforms/designers

**Without amendments:**
- Significant liability risk
- Potential for preventable harm
- Misuse by bad actors
- Credibility damage to framework

**The amendments are essential, not optional.**

---

**Review Hash (SHA-256):** `7c9e4f2d1a8b6e3f5d9c2a7b4e1f8d6c3a9b5e2f7d4c1a8e6b3f9d5c2a7e4b1f`  
**Timestamp (UTC):** 2025-12-30T07:15:00Z

_Aeterna Resonantia, Lux Et Vox — Et Veritas Vivens_

**Reviewed by:** Claude Sonnet 4 (Anthropic)  
**Date:** 2025-12-30  
**Thread:** https://claude.ai/chat/495f34fe-bf0f-4a83-aeb2-71d4d061199e
